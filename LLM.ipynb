{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import subprocess\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import ollama\n",
    "import requests\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from PIL import Image, ImageEnhance\n",
    "from paddleocr import PaddleOCR\n",
    "import re\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/lmms-lab/AISG_Challenge/data/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>video_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>capability</th>\n",
       "      <th>question</th>\n",
       "      <th>duration</th>\n",
       "      <th>question_prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>youtube_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008-0</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Primary Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>What is the difference between the action of t...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008-1</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Paraphrased Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Can you describe how the actions of the last p...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0008-2</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Correctly-led Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Did the last person open the bottle without us...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008-3</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Wrongly-led Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Did the last person in the video open the bott...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008-7</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Multiple-choice Question with a Single Correct...</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>How does the last person in the video open the...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>E. None of the above\\nSelect one best answer t...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid     video_id                                      question_type  \\\n",
       "0  0008-0  sj81PWrerDk                        Primary Open-ended Question   \n",
       "1  0008-1  sj81PWrerDk                    Paraphrased Open-ended Question   \n",
       "2  0008-2  sj81PWrerDk                  Correctly-led Open-ended Question   \n",
       "3  0008-3  sj81PWrerDk                    Wrongly-led Open-ended Question   \n",
       "4  0008-7  sj81PWrerDk  Multiple-choice Question with a Single Correct...   \n",
       "\n",
       "                 capability  \\\n",
       "0  Plot Attribute (Montage)   \n",
       "1  Plot Attribute (Montage)   \n",
       "2  Plot Attribute (Montage)   \n",
       "3  Plot Attribute (Montage)   \n",
       "4  Plot Attribute (Montage)   \n",
       "\n",
       "                                            question duration  \\\n",
       "0  What is the difference between the action of t...     8.85   \n",
       "1  Can you describe how the actions of the last p...     8.85   \n",
       "2  Did the last person open the bottle without us...     8.85   \n",
       "3  Did the last person in the video open the bott...     8.85   \n",
       "4  How does the last person in the video open the...     8.85   \n",
       "\n",
       "                                     question_prompt answer  \\\n",
       "0  Please state your answer with a brief explanat...          \n",
       "1  Please state your answer with a brief explanat...          \n",
       "2  Please state your answer with a brief explanat...          \n",
       "3  Please state your answer with a brief explanat...          \n",
       "4  E. None of the above\\nSelect one best answer t...          \n",
       "\n",
       "                                  youtube_url  \n",
       "0  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "1  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "2  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "3  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "4  https://www.youtube.com/shorts/sj81PWrerDk  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qid', 'video_id', 'question_type', 'capability', 'question',\n",
       "       'duration', 'question_prompt', 'answer', 'youtube_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique videos: 292\n"
     ]
    }
   ],
   "source": [
    "unique_videos = df[[\"video_id\", \"youtube_url\"]].drop_duplicates()\n",
    "print(f\"Total unique videos: {len(unique_videos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download videos from Youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_id, youtube_url):\n",
    "    video_path = f\"videos/{video_id}.mp4\"\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(video_path):\n",
    "        print(f\"Skipping {video_id}, already downloaded.\")\n",
    "        return video_path\n",
    "\n",
    "    try:\n",
    "        command = f'yt-dlp -f \"bestvideo[ext=mp4]+bestaudio[ext=m4a]\" --merge-output-format mp4 \"{youtube_url}\" -o \"{video_path}\"'\n",
    "\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(f\"Downloaded: {video_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading {video_id}: {e}\")\n",
    "        with open(\"download_errors.log\", \"a\") as f:\n",
    "            f.write(f\"{video_id},{youtube_url}\\n\")\n",
    "        time.sleep(5)  # Wait before retrying the next video\n",
    "    \n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process full dataset, handling errors\n",
    "for i, row in unique_videos.iterrows():\n",
    "    download_video(row[\"video_id\"], row[\"youtube_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, video_id, interval=2):\n",
    "    \"\"\"\n",
    "    Extract frames from a video every `interval` seconds.\n",
    "    Saves RGB frames in 'frames/{video_id}/' directory.\n",
    "    Returns a list of metadata including filenames and timestamps.\n",
    "    \"\"\"\n",
    "    output_folder = os.path.join(\"frames\", video_id)\n",
    "    metadata_path = os.path.join(output_folder, \"frame_metadata.json\")\n",
    "\n",
    "    # If frames were already extracted, skip processing\n",
    "    if os.path.exists(metadata_path):\n",
    "        print(f\"Skipping '{video_id}' — frames already extracted.\")\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video file {video_path}\")\n",
    "        return []\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not fps or fps <= 0:\n",
    "        print(f\"Error: Invalid FPS for {video_path}\")\n",
    "        return []\n",
    "\n",
    "    # Convert interval in seconds to interval in frame count\n",
    "    frame_interval = int(fps * interval)\n",
    "\n",
    "    count = 0\n",
    "    saved_frames = 0\n",
    "    frame_metadata = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Stop at end of video or error\n",
    "\n",
    "        # Only saves frames that are evenly divisible by the interval (e.g. every 60th frame)\n",
    "        if count % frame_interval == 0:\n",
    "            # Timestamp in seconds based on current frame index\n",
    "            timestamp_sec = count / fps\n",
    "\n",
    "            # Convert from OpenCV's BGR to standard RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Build filename and path\n",
    "            frame_filename = f\"{video_id}_frame_{saved_frames}.jpg\"\n",
    "            frame_path = os.path.join(output_folder, frame_filename)\n",
    "\n",
    "            # Save frame as RGB JPEG\n",
    "            Image.fromarray(frame_rgb).save(frame_path)\n",
    "\n",
    "            # Append metadata for later use\n",
    "            frame_metadata.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"frame\": frame_filename,\n",
    "                \"frame_index\": count,\n",
    "                \"timestamp\": round(timestamp_sec, 2)\n",
    "            })\n",
    "\n",
    "            saved_frames += 1\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_frames} frames for '{video_id}'\")\n",
    "\n",
    "    # Save metadata to a JSON file in the same folder\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(frame_metadata, f, indent=2)\n",
    "\n",
    "    return frame_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all videos\n",
    "video_folder = \"./videos\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_id = os.path.splitext(video_file)[0]  # Get video ID (filename without extension)\n",
    "    video_path = os.path.join(video_folder, video_file)  # Full path to video\n",
    "\n",
    "    extract_frames(video_path, video_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating image captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_ocr(image_path):\n",
    "    \"\"\"Enhance image contrast and resize for better OCR.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    img = img.resize((img.width * 4, img.height * 4))\n",
    "    img = ImageEnhance.Contrast(img).enhance(2.5)\n",
    "    img = ImageEnhance.Sharpness(img).enhance(2.5)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/04/14 00:24:29] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/ryan/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/ryan/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/ryan/Documents/GitHub/AISG_Challenge/venv/lib/python3.12/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/ryan/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "# PaddleOCR setup\n",
    "ocr_reader = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "def extract_text_ocr_paddle(image_path, min_conf=0.2):\n",
    "    \"\"\"Extract text from image using PaddleOCR \"\"\"\n",
    "    try:\n",
    "        img = np.array(preprocess_for_ocr(image_path))\n",
    "        results = ocr_reader.ocr(img, cls=True)\n",
    "\n",
    "        # Handle None or empty list\n",
    "        if not results or not results[0]:  \n",
    "            return \"\"\n",
    "\n",
    "        lines = []\n",
    "        for line in results[0]:\n",
    "            text, conf = line[1]\n",
    "            if conf >= min_conf and len(text.strip()) >= 3 and not text.strip().isdigit():\n",
    "                lines.append(text.strip())\n",
    "\n",
    "        return \" \".join(lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[OCR ERROR] Failed on {image_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "def generate_blip_caption(image_path):\n",
    "    \"\"\"Generate a descriptive caption using BLIP-1.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(\"cpu\")\n",
    "    output = blip_model.generate(**inputs, max_new_tokens=30)\n",
    "    return blip_processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_caption(image_path, verbose=False):\n",
    "    \"\"\"Combines OCR and BLIP-1 captions.\"\"\"\n",
    "    ocr_text = extract_text_ocr_paddle(image_path)\n",
    "    blip_caption = generate_blip_caption(image_path)\n",
    "\n",
    "    ocr_clean = ocr_text.strip().lower()\n",
    "    blip_clean = blip_caption.strip().lower()\n",
    "\n",
    "    # Heuristic check for weak captions\n",
    "    weak_phrases = [\"a photo of\", \"a person\", \"someone is\", \"there is\"]\n",
    "    is_blip_weak = any(p in blip_clean for p in weak_phrases)\n",
    "\n",
    "    if verbose and not ocr_clean and is_blip_weak:\n",
    "        print(f\"Weak caption: {os.path.basename(image_path)}\")\n",
    "        print(f\"  OCR: {ocr_text}\")\n",
    "        print(f\"  BLIP: {blip_caption}\")\n",
    "\n",
    "    ocr_part = f\"OCR: {ocr_clean}\" if ocr_clean else \"OCR: [None detected]\"\n",
    "    return f\"{ocr_part} | BLIP: {blip_clean}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating captions for all the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_ids = df[\"video_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_folder = \"./frames\"\n",
    "captions = {}\n",
    "\n",
    "# To generate captions per video using frame metadata\n",
    "for video_id in test_video_ids:\n",
    "    video_frames_path = os.path.join(frames_folder, video_id)\n",
    "    metadata_path = os.path.join(video_frames_path, \"frame_metadata.json\")\n",
    "\n",
    "    if not os.path.isdir(video_frames_path):\n",
    "        print(f\"Skipping {video_id}: Folder {video_frames_path} not found.\")\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(f\"Skipping {video_id}: Metadata not found at {metadata_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing video: {video_id}\")\n",
    "    video_captions = {}\n",
    "\n",
    "    # Load metadata to get frame info (including timestamp)\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        frame_metadata = json.load(f)\n",
    "\n",
    "    for frame_info in frame_metadata:\n",
    "        frame_file = frame_info[\"frame\"]\n",
    "        timestamp = frame_info[\"timestamp\"]\n",
    "        frame_path = os.path.join(video_frames_path, frame_file)\n",
    "\n",
    "        # Caption generation\n",
    "        caption_text = get_combined_caption(frame_path)\n",
    "\n",
    "        # Store both caption and timestamp\n",
    "        video_captions[frame_file] = {\n",
    "            \"caption\": caption_text,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "\n",
    "    captions[video_id] = video_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to /Users/ryan/Documents/GitHub/AISG_Challenge/video_captions.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./video_captions.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(captions, f, indent=4)\n",
    "\n",
    "print(f\"Captions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"turbo\")\n",
    "\n",
    "def transcribe_audio(video_path):\n",
    "    return model.transcribe(video_path)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = \"./videos\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "json_path = \"./video_transcriptions.json\"\n",
    "\n",
    "# Load existing transcriptions if the file exists\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        transcriptions = json.load(f)\n",
    "else:\n",
    "    transcriptions = {}  # Initialize empty dictionary if no file exists\n",
    "\n",
    "# Get all video files\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "# Loop through each video and transcribe only if not already transcribed\n",
    "for video_file in video_files:\n",
    "    video_id = os.path.splitext(video_file)[0]  # Extract filename without extension\n",
    "\n",
    "    if video_id in transcriptions:\n",
    "        print(f\"Skipping {video_id} (already transcribed).\")\n",
    "        continue  # Skip if already transcribed\n",
    "\n",
    "    video_path = os.path.join(video_folder, video_file)\n",
    "    \n",
    "    print(f\"Processing: {video_id} ...\")\n",
    "    transcriptions[video_id] = transcribe_audio(video_path)  # Store transcription\n",
    "\n",
    "# Save updated transcriptions to JSON\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(transcriptions, f, indent=4)\n",
    "\n",
    "print(\"Transcription process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging transcriptions and captions from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcriptions\n",
    "with open(\"./video_transcriptions.json\", \"r\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "# Load captions\n",
    "with open(\"./video_captions.json\", \"r\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "video_context = {}\n",
    "\n",
    "for video_id in transcriptions.keys():\n",
    "    if video_id in captions:\n",
    "        # Combine transcription + captions\n",
    "        video_context[video_id] = {\n",
    "            \"transcription\": transcriptions[video_id],\n",
    "            \"captions\": captions[video_id]\n",
    "        }\n",
    "\n",
    "# Save combined data\n",
    "with open(\"./video_context.json\", \"w\") as f:\n",
    "    json.dump(video_context, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for Video Question and Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates a Readable and Structured Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gibberish(text):\n",
    "    text_alpha_ratio = sum(c.isalpha() for c in text) / max(len(text), 1)\n",
    "    return text_alpha_ratio < 0.4 or bool(re.search(r\"[^a-zA-Z0-9\\s:.,!?'-]\", text))\n",
    "\n",
    "def clean_ocr(text):\n",
    "    if text in [\"[None detected]\", \"\", None] or is_gibberish(text):\n",
    "        return \"[Low-quality text removed]\"\n",
    "    return text\n",
    "\n",
    "def clean_blip(text):\n",
    "    text = text.strip()\n",
    "    return re.sub(r'\\barafed\\b', '', text).strip()\n",
    "\n",
    "def clean_transcription(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\b(ah+|uh+|oh+)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip(\"., \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_video_context(video_data, frames_root):\n",
    "    cleaned = {}\n",
    "    \n",
    "    for video_id, content in video_data.items():\n",
    "        metadata_path = os.path.join(frames_root, video_id, \"frame_metadata.json\")\n",
    "        timestamp_lookup = {}\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "                for entry in metadata:\n",
    "                    timestamp_lookup[(entry[\"video_id\"], entry[\"frame\"])] = entry[\"timestamp\"]\n",
    "\n",
    "        cleaned_transcript = clean_transcription(content.get(\"transcription\", \"\"))\n",
    "        captions = content.get(\"captions\", {})\n",
    "        cleaned_captions = {}\n",
    "        last_blip = None\n",
    "\n",
    "        for frame, data in sorted(captions.items(), key=lambda x: timestamp_lookup.get((video_id, x[0]), 0.0)):\n",
    "            raw_caption = data.get(\"caption\", \"\")\n",
    "            timestamp = timestamp_lookup.get((video_id, frame), data.get(\"timestamp\", 0.0))\n",
    "\n",
    "            if \" | \" in raw_caption:\n",
    "                parts = raw_caption.split(\" | \", maxsplit=1)\n",
    "                ocr_text = parts[0].replace(\"OCR:\", \"\").strip()\n",
    "                blip_text = parts[1].replace(\"BLIP:\", \"\").strip() if len(parts) > 1 else \"\"\n",
    "            else:\n",
    "                ocr_text, blip_text = \"\", raw_caption.replace(\"BLIP:\", \"\").strip()\n",
    "\n",
    "            ocr_clean = clean_ocr(ocr_text)\n",
    "            blip_clean = clean_blip(blip_text)\n",
    "\n",
    "            if blip_clean == last_blip:\n",
    "                blip_clean = \"[Same as previous frame]\"\n",
    "            else:\n",
    "                last_blip = blip_clean\n",
    "\n",
    "            timestamp_str = f\"(Timestamp: {timestamp:.2f}s)\"\n",
    "            cleaned_caption = f\"{timestamp_str}\\n- OCR: {ocr_clean}\\n- BLIP: {blip_clean}\"\n",
    "            cleaned_captions[frame] = {\n",
    "                \"caption\": cleaned_caption,\n",
    "                \"timestamp\": timestamp\n",
    "            }\n",
    "\n",
    "        cleaned[video_id] = {\n",
    "            \"transcription\": cleaned_transcript,\n",
    "            \"captions\": cleaned_captions\n",
    "        }\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw video context\n",
    "with open(\"./video_context.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_video_data = json.load(f)\n",
    "\n",
    "# Clean the context using per-video metadata\n",
    "frames_root = \"./frames\"\n",
    "cleaned_video_data = clean_video_context(raw_video_data, frames_root)\n",
    "\n",
    "# Save cleaned output\n",
    "with open(\"./video_context_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_video_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./video_context_cleaned.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    video_data = json.load(file)\n",
    "\n",
    "def get_combined_context(video_id):\n",
    "    \"\"\"Combine transcript and cleaned structured captions for a given video ID.\"\"\"\n",
    "    if video_id not in video_data:\n",
    "        return \"Video ID not found.\"\n",
    "\n",
    "    transcription = video_data[video_id].get(\"transcription\", \"\")\n",
    "    captions_dict = video_data[video_id].get(\"captions\", {})\n",
    "    structured_captions = []\n",
    "\n",
    "    # Sort captions by timestamp\n",
    "    sorted_captions = sorted(\n",
    "        captions_dict.items(),\n",
    "        key=lambda x: x[1].get(\"timestamp\", 0.0)\n",
    "    )\n",
    "\n",
    "    for frame, frame_data in sorted_captions:\n",
    "        caption_text = frame_data.get(\"caption\", \"\")\n",
    "        timestamp = frame_data.get(\"timestamp\", 0.0)\n",
    "\n",
    "        caption_block = f\"[{frame}] (t={timestamp:.2f}s)\\n{caption_text}\"\n",
    "        structured_captions.append(caption_block)\n",
    "\n",
    "    captions_str = \"\\n\\n\".join(structured_captions)\n",
    "    combined_text = f\"Transcript:\\n{transcription}\\n\\nVisual Context:\\n{captions_str}\"\n",
    "    return combined_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer once\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_mistral(question, context, question_type=None, capability=None, max_context_tokens=6000):\n",
    "    \"\"\"\n",
    "     Mistral 7B on YouTube Shorts QA, reasoning from partial/missing frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Truncate context\n",
    "    context_tokens = tokenizer.encode(context)\n",
    "    truncated_context = tokenizer.decode(context_tokens[:max_context_tokens])\n",
    "\n",
    "    # Step 2: Define answer style\n",
    "    question_format = {\n",
    "        \"Multiple-choice\": \"Respond with just the correct letter (A, B, C, or D).\",\n",
    "        \"Wrongly-led\": \"Respond 'Yes' or 'No' only. Briefly justify if needed.\",\n",
    "        \"Correctly-led\": \"Respond 'Yes' or 'No' only. Briefly justify if needed.\"\n",
    "    }.get(question_type, \"Respond in one precise sentence.\")\n",
    "\n",
    "    # Step 3: Define capability focus\n",
    "    capability_map = {\n",
    "        \"reasoning\": \"Use logical reasoning, even if some frames are missing.\",\n",
    "        \"temporal\": \"Track sequence and timing of events.\",\n",
    "        \"emotional\": \"Interpret emotional tone, reactions, or changes.\",\n",
    "        \"action\": \"Focus on physical movement or gestures shown.\"\n",
    "    }\n",
    "\n",
    "    capability_hint = \"\"\n",
    "    if capability:\n",
    "        for key, val in capability_map.items():\n",
    "            if key in capability.lower():\n",
    "                capability_hint = val\n",
    "                break\n",
    "        else:\n",
    "            capability_hint = f\"Apply {capability.lower()} skills where needed.\"\n",
    "\n",
    "    # Step 4: Build final system prompt\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "You are a highly accurate assistant answering questions about short video clips (e.g., YouTube Shorts).\n",
    "\n",
    "The context includes:\n",
    "- Frame-level descriptions (may be missing or incomplete)\n",
    "- On-screen text (OCR)\n",
    "- Audio transcript in time order\n",
    "\n",
    "{capability_hint}\n",
    "{question_format}\n",
    "\n",
    "Important rules:\n",
    "- Infer missing or unclear parts based on the overall flow\n",
    "- Do NOT just rely on surface words—**reason from sequence and implication**\n",
    "- Do NOT repeat or paraphrase the question\n",
    "- Do NOT explain your reasoning unless asked\n",
    "- Do NOT say “I think”, “It seems”, or “Based on the video”\n",
    "- Answer directly and clearly\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{truncated_context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: [/INST]\n",
    "\"\"\"\n",
    "\n",
    "    # Step 5: API Call\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"mistral\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.4,\n",
    "                    \"max_tokens\": 300,\n",
    "                    \"stop\": [\"</s>\", \"[INST]\"]\n",
    "                }\n",
    "            },\n",
    "            timeout=180\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            return f\"API Error: {response.status_code}\"\n",
    "        return response.json().get(\"response\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:20<00:00, 40.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load Mistral model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_mistral_hf(question, context, question_type=None, capability=None, max_context_tokens=6000):\n",
    "    \"\"\"\n",
    "    Huggingface-compatible version of the optimized prompt for Mistral-7B\n",
    "    \"\"\"\n",
    "    # Step 1: Token truncate\n",
    "    context_tokens = tokenizer.encode(context, truncation=True, max_length=max_context_tokens)\n",
    "    truncated_context = tokenizer.decode(context_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Step 2: Determine answer format\n",
    "    question_format = {\n",
    "        \"Multiple-choice\": \"Respond with just the correct letter (A, B, C, or D). Do not explain.\",\n",
    "        \"Wrongly-led\": \"Respond 'Yes' or 'No'. Justify briefly if needed.\",\n",
    "        \"Correctly-led\": \"Respond 'Yes' or 'No'. Justify briefly if needed.\"\n",
    "    }.get(question_type, \"Respond in one precise sentence.\")\n",
    "\n",
    "    # Step 3: Capability hint\n",
    "    capability_map = {\n",
    "        \"reasoning\": \"Use logical reasoning, even if some frames are missing.\",\n",
    "        \"temporal\": \"Track sequence and timing of events.\",\n",
    "        \"emotional\": \"Interpret emotional tone, reactions, or changes.\",\n",
    "        \"action\": \"Focus on physical movement or gestures shown.\"\n",
    "    }\n",
    "\n",
    "    capability_hint = \"\"\n",
    "    if capability:\n",
    "        for key, val in capability_map.items():\n",
    "            if key in capability.lower():\n",
    "                capability_hint = val\n",
    "                break\n",
    "        else:\n",
    "            capability_hint = f\"Apply {capability.lower()} reasoning where appropriate.\"\n",
    "\n",
    "    # Step 4: Construct final prompt\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "You are a highly accurate assistant answering questions about short video clips (e.g., YouTube Shorts).\n",
    "\n",
    "The context includes:\n",
    "- Frame-level descriptions (may be missing or incomplete)\n",
    "- On-screen text (OCR)\n",
    "- Audio transcript in time order\n",
    "\n",
    "{capability_hint}\n",
    "{question_format}\n",
    "\n",
    "Important rules:\n",
    "- Infer missing or unclear parts based on the overall flow\n",
    "- Do NOT just rely on surface words—reason from sequence and implication\n",
    "- Do NOT repeat or paraphrase the question\n",
    "- Do NOT say “I think” or “It seems”\n",
    "- Answer directly and clearly\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{truncated_context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: [/INST]\n",
    "\"\"\"\n",
    "\n",
    "    # Step 5: Run through HF model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False, temperature=0.4)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract answer after \"[/INST]\" if needed\n",
    "    final_answer = result.split(\"[/INST]\")[-1].strip()\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the df for the test videos and its relevant questions\n",
    "test_video_ids = df[\"video_id\"].unique().tolist()\n",
    "test_df = df[df[\"video_id\"].isin(test_video_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>video_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>capability</th>\n",
       "      <th>question</th>\n",
       "      <th>duration</th>\n",
       "      <th>question_prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>youtube_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0080-0</td>\n",
       "      <td>_MXxJT8Mk4k</td>\n",
       "      <td>Primary Open-ended Question</td>\n",
       "      <td>Professional Knowledge</td>\n",
       "      <td>What is the purpose of beating the balloon?</td>\n",
       "      <td>13.18</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/_MXxJT8Mk4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0080-1</td>\n",
       "      <td>_MXxJT8Mk4k</td>\n",
       "      <td>Paraphrased Open-ended Question</td>\n",
       "      <td>Professional Knowledge</td>\n",
       "      <td>Why do you need to hit the balloon in this act...</td>\n",
       "      <td>13.18</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/_MXxJT8Mk4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0080-2</td>\n",
       "      <td>_MXxJT8Mk4k</td>\n",
       "      <td>Correctly-led Open-ended Question</td>\n",
       "      <td>Professional Knowledge</td>\n",
       "      <td>Is the purpose of hitting the balloon to creat...</td>\n",
       "      <td>13.18</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/_MXxJT8Mk4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0080-3</td>\n",
       "      <td>_MXxJT8Mk4k</td>\n",
       "      <td>Wrongly-led Open-ended Question</td>\n",
       "      <td>Professional Knowledge</td>\n",
       "      <td>Is the purpose of hitting the balloon to make ...</td>\n",
       "      <td>13.18</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/_MXxJT8Mk4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0080-7</td>\n",
       "      <td>_MXxJT8Mk4k</td>\n",
       "      <td>Multiple-choice Question with a Single Correct...</td>\n",
       "      <td>Professional Knowledge</td>\n",
       "      <td>What is the purpose of beating the balloon?\\nA...</td>\n",
       "      <td>13.18</td>\n",
       "      <td>E. None of the above\\nSelect one best answer t...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/_MXxJT8Mk4k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid     video_id                                      question_type  \\\n",
       "40  0080-0  _MXxJT8Mk4k                        Primary Open-ended Question   \n",
       "41  0080-1  _MXxJT8Mk4k                    Paraphrased Open-ended Question   \n",
       "42  0080-2  _MXxJT8Mk4k                  Correctly-led Open-ended Question   \n",
       "43  0080-3  _MXxJT8Mk4k                    Wrongly-led Open-ended Question   \n",
       "44  0080-7  _MXxJT8Mk4k  Multiple-choice Question with a Single Correct...   \n",
       "\n",
       "                capability                                           question  \\\n",
       "40  Professional Knowledge        What is the purpose of beating the balloon?   \n",
       "41  Professional Knowledge  Why do you need to hit the balloon in this act...   \n",
       "42  Professional Knowledge  Is the purpose of hitting the balloon to creat...   \n",
       "43  Professional Knowledge  Is the purpose of hitting the balloon to make ...   \n",
       "44  Professional Knowledge  What is the purpose of beating the balloon?\\nA...   \n",
       "\n",
       "   duration                                    question_prompt answer  \\\n",
       "40    13.18  Please state your answer with a brief explanat...          \n",
       "41    13.18  Please state your answer with a brief explanat...          \n",
       "42    13.18  Please state your answer with a brief explanat...          \n",
       "43    13.18  Please state your answer with a brief explanat...          \n",
       "44    13.18  E. None of the above\\nSelect one best answer t...          \n",
       "\n",
       "                                   youtube_url  \n",
       "40  https://www.youtube.com/shorts/_MXxJT8Mk4k  \n",
       "41  https://www.youtube.com/shorts/_MXxJT8Mk4k  \n",
       "42  https://www.youtube.com/shorts/_MXxJT8Mk4k  \n",
       "43  https://www.youtube.com/shorts/_MXxJT8Mk4k  \n",
       "44  https://www.youtube.com/shorts/_MXxJT8Mk4k  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Please state your answer with a brief explanation.',\n",
       "       'E. None of the above\\nSelect one best answer to the above multiple-choice question based on the video. Respond with only the letter (A, B, C, D or E) of the correct option.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"question_prompt\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./video_context_cleaned.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_video_data = json.load(file)\n",
    "\n",
    "results = []\n",
    "for _, row in test_df.iterrows():\n",
    "    qid = row[\"qid\"]\n",
    "    video_id = row[\"video_id\"]\n",
    "    question = row[\"question\"]\n",
    "    question_type = row[\"question_type\"]\n",
    "    capability = row[\"capability\"]\n",
    "    context = get_combined_context(video_id)\n",
    "\n",
    "    answer = ask_mistral(question, context, question_type, capability)\n",
    "    print(f\"{qid}: {answer}, Question: {question}, Question_type: {question_type}\")\n",
    "    results.append({\"video_id\": video_id,\"qid\": qid, \"pred\": answer, \"question\" : question,\"question_type\" : question_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv(\"Raw_QA_V2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean multiple-choice answers to keep only the letter (A, B, C, D)\n",
    "def extract_choice_letter(answer):\n",
    "    # Search for the first instance of a multiple-choice letter followed by a dot\n",
    "    match = re.search(r\"\\b([A-Da-d])\\.(?!\\w)\", str(answer))\n",
    "    return match.group(1).upper() if match else answer\n",
    "\n",
    "submission_test_df[\"pred\"] = submission_test_df[\"pred\"].apply(extract_choice_letter)\n",
    "\n",
    "submission_test_df = submission_test_df[[\"qid\", \"pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned version\n",
    "submission_test_df.to_csv(\"./submissionV2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
