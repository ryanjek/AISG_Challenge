{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import subprocess\n",
    "import pytesseract\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import ollama\n",
    "import requests\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/lmms-lab/AISG_Challenge/data/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>video_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>capability</th>\n",
       "      <th>question</th>\n",
       "      <th>duration</th>\n",
       "      <th>question_prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>youtube_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008-0</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Primary Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>What is the difference between the action of t...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008-1</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Paraphrased Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Can you describe how the actions of the last p...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0008-2</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Correctly-led Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Did the last person open the bottle without us...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008-3</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Wrongly-led Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Did the last person in the video open the bott...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008-7</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Multiple-choice Question with a Single Correct...</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>How does the last person in the video open the...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>E. None of the above\\nSelect one best answer t...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid     video_id                                      question_type  \\\n",
       "0  0008-0  sj81PWrerDk                        Primary Open-ended Question   \n",
       "1  0008-1  sj81PWrerDk                    Paraphrased Open-ended Question   \n",
       "2  0008-2  sj81PWrerDk                  Correctly-led Open-ended Question   \n",
       "3  0008-3  sj81PWrerDk                    Wrongly-led Open-ended Question   \n",
       "4  0008-7  sj81PWrerDk  Multiple-choice Question with a Single Correct...   \n",
       "\n",
       "                 capability  \\\n",
       "0  Plot Attribute (Montage)   \n",
       "1  Plot Attribute (Montage)   \n",
       "2  Plot Attribute (Montage)   \n",
       "3  Plot Attribute (Montage)   \n",
       "4  Plot Attribute (Montage)   \n",
       "\n",
       "                                            question duration  \\\n",
       "0  What is the difference between the action of t...     8.85   \n",
       "1  Can you describe how the actions of the last p...     8.85   \n",
       "2  Did the last person open the bottle without us...     8.85   \n",
       "3  Did the last person in the video open the bott...     8.85   \n",
       "4  How does the last person in the video open the...     8.85   \n",
       "\n",
       "                                     question_prompt answer  \\\n",
       "0  Please state your answer with a brief explanat...          \n",
       "1  Please state your answer with a brief explanat...          \n",
       "2  Please state your answer with a brief explanat...          \n",
       "3  Please state your answer with a brief explanat...          \n",
       "4  E. None of the above\\nSelect one best answer t...          \n",
       "\n",
       "                                  youtube_url  \n",
       "0  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "1  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "2  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "3  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "4  https://www.youtube.com/shorts/sj81PWrerDk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique videos: 292\n"
     ]
    }
   ],
   "source": [
    "unique_videos = df[[\"video_id\", \"youtube_url\"]].drop_duplicates()\n",
    "print(f\"Total unique videos: {len(unique_videos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download videos from Youtube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_id, youtube_url):\n",
    "    video_path = f\"videos/{video_id}.mp4\"\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(video_path):\n",
    "        print(f\"Skipping {video_id}, already downloaded.\")\n",
    "        return video_path\n",
    "\n",
    "    try:\n",
    "        command = f'yt-dlp -f \"bestvideo[ext=mp4]+bestaudio[ext=m4a]\" --merge-output-format mp4 \"{youtube_url}\" -o \"{video_path}\"'\n",
    "\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(f\"Downloaded: {video_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading {video_id}: {e}\")\n",
    "        with open(\"download_errors.log\", \"a\") as f:\n",
    "            f.write(f\"{video_id},{youtube_url}\\n\")\n",
    "        time.sleep(5)  # Wait before retrying the next video\n",
    "    \n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process full dataset, handling errors\n",
    "for i, row in unique_videos.iterrows():\n",
    "    download_video(row[\"video_id\"], row[\"youtube_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, video_id, interval=2):\n",
    "    \"\"\"\n",
    "    Extract frames from a video every `interval` seconds.\n",
    "    Saves frames in 'frames/{video_id}/' directory.\n",
    "    \"\"\"\n",
    "    output_folder = f\"frames/{video_id}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Cannot open video file {video_path}\")\n",
    "        return\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get video FPS\n",
    "    if fps is None or fps == 0:\n",
    "        print(f\"Error: Could not retrieve FPS for {video_path}\")\n",
    "        return\n",
    "    \n",
    "    frame_interval = int(fps * interval)  # Convert seconds to frame count\n",
    "    \n",
    "    count = 0\n",
    "    saved_frames = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Stop if video ends\n",
    "        \n",
    "        if count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"{video_id}_frame_{saved_frames}.jpg\")\n",
    "            Image.fromarray(frame).save(frame_filename)  # Save frame as image\n",
    "            saved_frames += 1\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_frames} frames for {video_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 30 frames for XeUbzTntoW0\n"
     ]
    }
   ],
   "source": [
    "# Testing on 1 video\n",
    "video_folder = \"/Users/ryan/Documents/GitHub/AISG_Challenge/videos\"\n",
    "video_id = \"XeUbzTntoW0\"\n",
    "video_path = os.path.join(video_folder, f\"{video_id}.mp4\") \n",
    "\n",
    "extract_frames(video_path, video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all videos\n",
    "video_folder = \"/Users/ryan/Documents/GitHub/AISG_Challenge/videos\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_id = os.path.splitext(video_file)[0]  # Get video ID (filename without extension)\n",
    "    video_path = os.path.join(video_folder, video_file)  # Full path to video\n",
    "\n",
    "    extract_frames(video_path, video_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating image captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_ocr(image_path):\n",
    "    \"\"\"Extract on-frame text using OCR (Tesseract).\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    extracted_text = pytesseract.image_to_string(image, lang=\"eng\") \n",
    "    return extracted_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_generate_caption(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "    caption = blip_model.generate(**inputs)\n",
    "    return blip_processor.decode(caption[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_caption(image_path):\n",
    "    \"\"\"Extract text from image (OCR) and generate a caption (BLIP).\"\"\"\n",
    "    ocr_text = extract_text_ocr(image_path)\n",
    "    blip_caption = blip_generate_caption(image_path)\n",
    "\n",
    "    if ocr_text:\n",
    "        return f\"OCR Extracted: '{ocr_text}' | BLIP Caption: '{blip_caption}'\"\n",
    "    return f\"BLIP Caption: '{blip_caption}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions Generated!\n"
     ]
    }
   ],
   "source": [
    "# Testing on 1 video\n",
    "frames_folder = \"/Users/ryan/Documents/GitHub/AISG_Challenge/frames\"\n",
    "video_id = \"XeUbzTntoW0\"  # Specify the single video ID you want to process\n",
    "\n",
    "video_frames_path = os.path.join(frames_folder, video_id)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(video_frames_path):\n",
    "    print(f\"Error: Video frames folder {video_frames_path} does not exist.\")\n",
    "else:\n",
    "    captions = {}\n",
    "    video_captions = {}\n",
    "\n",
    "    # Process each frame\n",
    "    for frame in sorted(os.listdir(video_frames_path)):  # Sort to maintain order\n",
    "        frame_path = os.path.join(video_frames_path, frame)\n",
    "\n",
    "        if not frame.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Skip non-image files\n",
    "            continue\n",
    "\n",
    "        caption = get_combined_caption(frame_path)\n",
    "        video_captions[frame] = caption  # Store caption\n",
    "\n",
    "    captions[video_id] = video_captions  # Store all captions for the video\n",
    "\n",
    "    print(\"Captions Generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to video_captions.json!\n"
     ]
    }
   ],
   "source": [
    "# Testing on 1 video\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/test_video_captions.json\", \"w\") as f:\n",
    "    json.dump(captions, f, indent=4)\n",
    "\n",
    "print(\"Captions saved to video_captions.json!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_folder = \"/Users/ryan/Documents/GitHub/AISG_Challenge/frames\"\n",
    "captions = {}\n",
    "\n",
    "# Loop through all video frame folders\n",
    "for video_id in os.listdir(frames_folder):\n",
    "    video_frames_path = os.path.join(frames_folder, video_id)\n",
    "    \n",
    "    # Skip non-folder items\n",
    "    if not os.path.isdir(video_frames_path):\n",
    "        continue\n",
    "    \n",
    "    # Process each frame\n",
    "    video_captions = {}\n",
    "    for frame in sorted(os.listdir(video_frames_path)):  # Sort to maintain order\n",
    "        frame_path = os.path.join(video_frames_path, frame)\n",
    "        \n",
    "        if not frame.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Skip non-image files\n",
    "            continue\n",
    "        \n",
    "        caption = blip_generate_caption(frame_path)\n",
    "        video_captions[frame] = caption  # Store caption\n",
    "    \n",
    "    captions[video_id] = video_captions  # Store all captions for the video\n",
    "\n",
    "print(\"Captions Generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to video_captions.json!\n"
     ]
    }
   ],
   "source": [
    "# Save captions to a JSON file\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/video_captions.json\", \"w\") as f:\n",
    "    json.dump(captions, f, indent=4)\n",
    "\n",
    "print(\"Captions saved to video_captions.json!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"turbo\")\n",
    "\n",
    "def transcribe_audio(video_path):\n",
    "    return model.transcribe(video_path)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = \"/Users/ryan/Documents/GitHub/AISG_Challenge/videos\"\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "json_path = \"/Users/ryan/Documents/GitHub/AISG_Challenge/video_transcriptions.json\"\n",
    "\n",
    "# Load existing transcriptions if the file exists\n",
    "if os.path.exists(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        transcriptions = json.load(f)\n",
    "else:\n",
    "    transcriptions = {}  # Initialize empty dictionary if no file exists\n",
    "\n",
    "# Get all video files\n",
    "video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "\n",
    "# Loop through each video and transcribe only if not already transcribed\n",
    "for video_file in video_files:\n",
    "    video_id = os.path.splitext(video_file)[0]  # Extract filename without extension\n",
    "\n",
    "    if video_id in transcriptions:\n",
    "        print(f\"Skipping {video_id} (already transcribed).\")\n",
    "        continue  # Skip if already transcribed\n",
    "\n",
    "    video_path = os.path.join(video_folder, video_file)\n",
    "    \n",
    "    print(f\"Processing: {video_id} ...\")\n",
    "    transcriptions[video_id] = transcribe_audio(video_path)  # Store transcription\n",
    "\n",
    "# Save updated transcriptions to JSON\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(transcriptions, f, indent=4)\n",
    "\n",
    "print(\"Transcription process complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging transcriptions and captions from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged transcriptions and captions for testing!\n"
     ]
    }
   ],
   "source": [
    "# Load transcriptions\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/video_transcriptions.json\", \"r\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "# Load captions\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/video_captions.json\", \"r\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Merge data for 5 test videos\n",
    "video_context = {}\n",
    "\n",
    "for video_id in transcriptions.keys():\n",
    "    if video_id in captions:\n",
    "        # Combine transcription + captions\n",
    "        video_context[video_id] = {\n",
    "            \"transcription\": transcriptions[video_id],\n",
    "            \"captions\": captions[video_id]\n",
    "        }\n",
    "\n",
    "# Save combined data\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/video_context.json\", \"w\") as f:\n",
    "    json.dump(video_context, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on 1 video\n",
    "# Load transcriptions\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/video_transcriptions.json\", \"r\") as f:\n",
    "    transcriptions = json.load(f)\n",
    "\n",
    "# Load captions\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/test_video_captions.json\", \"r\") as f:\n",
    "    captions = json.load(f)\n",
    "\n",
    "# Merge data for 5 test videos\n",
    "video_context = {}\n",
    "\n",
    "for video_id in transcriptions.keys():\n",
    "    if video_id in captions:\n",
    "        # Combine transcription + captions\n",
    "        video_context[video_id] = {\n",
    "            \"transcription\": transcriptions[video_id],\n",
    "            \"captions\": captions[video_id]\n",
    "        }\n",
    "\n",
    "# Save combined data\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/test_video_context.json\", \"w\") as f:\n",
    "    json.dump(video_context, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for Video Question and Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining audio transcribe and frame captioning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Context Preview:\n",
      "Transcript:  Hey Daniel, would you mind cleaning your room? Sure. Okay Daniel, time to clean your room. No! Hi! Hey Daniel. Yep. Lunch will be ready in a minute. Okay. Hey Daniel, what's up? Hey Daniel, what's up? Hey Daniel, what's up? Lunch will be ready in a minute. Okay. Hey Daniel, look in mommy's eyes. Hey, one more minute and we're gonna have lunch, okay? Wake up baby brother! Here you go. Thank you. Here you go! Thank you. Thank you. Here you go! Thank you. All done. Okay. Alright. All done. This place will be better.\n",
      "\n",
      "Captions: there is a man that is standing in front of a counter there is a man standing in a kitchen looking at a counter there is a man that is sitting on a chair with a book araffe man in a living room with a pile of trash arafed man sitting in a chair reading a book there is a person laying on a bed with a pillow doorway view of a bathroom with a person standing in the doorway araffe in a mask is standing in front of a glass door there is a man sitting on a co\n"
     ]
    }
   ],
   "source": [
    "# Define the file path\n",
    "json_path = \"/Users/ryan/Documents/GitHub/AISG_Challenge/video_context.json\"\n",
    "\n",
    "# Load JSON data\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    video_data = json.load(file)\n",
    "\n",
    "def get_combined_context(video_id):\n",
    "    \"\"\"Combine transcript and captions for a given video ID.\"\"\"\n",
    "    if video_id not in video_data:\n",
    "        return \"Video ID not found.\"\n",
    "\n",
    "    # Get transcription\n",
    "    transcription = video_data[video_id].get(\"transcription\", \"\")\n",
    "\n",
    "    # Get captions\n",
    "    captions_dict = video_data[video_id].get(\"captions\", {})\n",
    "    captions = \" \".join(captions_dict.values())  # Combine all captions\n",
    "\n",
    "    # Combine both transcript and captions\n",
    "    combined_text = f\"Transcript: {transcription}\\n\\nCaptions: {captions}\"\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "# Example: Choose a video ID\n",
    "video_id = list(video_data.keys())[0]  # Pick first available video ID\n",
    "video_context = get_combined_context(video_id)\n",
    "\n",
    "print(\"\\nCombined Context Preview:\")\n",
    "print(video_context[:1000])  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many scenarios did the man show between the adult and the toddler?\n",
      "How many different scenarios did the man demonstrate between the adult and the toddler?\n",
      "Did the man show ten scenarios between the adult and the toddler?\n",
      "Did the man show six scenarios between the adult and the toddler?\n",
      "How many scenarios did the man show between the adult and the toddler?\n",
      "A. Ten\n",
      "B. Twelve\n",
      "C. Six\n",
      "D. Eight\n"
     ]
    }
   ],
   "source": [
    "questions = df[df[\"video_id\"] == \"XeUbzTntoW0\"][\"question\"]\n",
    "\n",
    "for q in questions:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on 1 video\n",
    "with open(\"/Users/ryan/Documents/GitHub/AISG_Challenge/test_video_context.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_video_data = json.load(file)\n",
    "\n",
    "test_video_id = list(test_video_data.keys())[0]\n",
    "test_video_context = get_combined_context(test_video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_deepseek_r1(question, context):\n",
    "    \"\"\"Ask DeepSeek R1 a question using the combined transcript + captions.\"\"\"\n",
    "    prompt = f\"{question}\\n\\nHere is the video context:\\n{context}\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"deepseek-r1\", prompt], \n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the video\n",
    "question = \"How many scenarios did the man show between the adult and the toddler?\"\n",
    "response = ask_deepseek_r1(question, video_context)\n",
    "\n",
    "print(\"\\n DeepSeek R1's Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising caption context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_context(context):\n",
    "    \"\"\"Summarize with proper prompt formatting and input handling.\"\"\"\n",
    "    # Create a clean, single-line prompt with [INST] tags\n",
    "    summary_prompt = (\n",
    "        f\"[INST] Summarize this video transcript concisely:\\n\\n{context}\\n[/INST]\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            [\"ollama\", \"run\", \"mistral\"],\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        # Send prompt via stdin instead of command argument\n",
    "        output, error = process.communicate(input=summary_prompt, timeout=120)\n",
    "        return output.strip() if output else context\n",
    "    except Exception as e:\n",
    "        print(f\"Summarization error: {str(e)}\")\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A man (Daniel) is asked multiple times by family members to clean his room, but resists.\n",
      "2. Lunch is prepared and served to someone else.\n",
      "3. Daniel looks at someone's eyes and wakes up another person.\n",
      "4. A person thanks someone for something.\n",
      "5. Daniel finishes cleaning his room, stating it will be better.\n",
      "6. Various scenes show other family members in different parts of the house (eating, reading, combing hair, skateboarding).\n"
     ]
    }
   ],
   "source": [
    "print(summarize_context(video_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral answering based on summarised context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def ask_mistral(question, context):\n",
    "    \"\"\"Robust Mistral implementation using HTTP API\"\"\"\n",
    "    summarized_context = summarize_context(context)\n",
    "    \n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "    You are an expert video analysis and Q&A model. Your task is to analyze video content based on:\n",
    "    {summarized_context}\n",
    "    <</SYS>>\n",
    "    \n",
    "    {question} [/INST]\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"mistral\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.3,\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"stop\": [\"</s>\", \"[INST]\"]\n",
    "                }\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"].strip()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "    except KeyError:\n",
    "        return \"Error: Invalid response format from API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mistral's Answer:\n",
      "1 scenario: The video shows a scenario involving an adult (presumably Daniel's parents or siblings) taking care of a baby brother, indicating that there is interaction between Daniel and his younger sibling. However, it does not provide specific instances where Daniel directly interacts with the toddler in a caregiving role, so I would count this as 1 scenario.\n"
     ]
    }
   ],
   "source": [
    "question = \"How many scenarios did the man show between the adult and the toddler?\"\n",
    "response = ask_mistral(question, video_context)\n",
    "\n",
    "print(\"\\nMistral's Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral without summarising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer once\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "def ask_mistral2(question, context, max_context_tokens=6000):\n",
    "    \"\"\"Use full context with token limit management\"\"\"\n",
    "\n",
    "    # Truncate context to fit model's token window while keeping meaningful information\n",
    "    context_tokens = tokenizer.encode(context)\n",
    "    truncated_context = tokenizer.decode(\n",
    "        context_tokens[:min(len(context_tokens), max_context_tokens)]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "    You are an expert in video analysis, capable of reconstructing a video's context based on extracted frames and captions. \n",
    "    The frames may contain partial information, so infer missing details using logical continuity while avoiding assumptions \n",
    "    beyond what is reasonable.\n",
    "\n",
    "    Given the following extracted frame descriptions, analyze the video holistically by considering:\n",
    "    - The sequence of events across frames (temporal continuity).\n",
    "    - Key actions, objects, and interactions present.\n",
    "    - Any patterns or logical conclusions that can be drawn from multiple frames.\n",
    "\n",
    "    If there are gaps in information, explicitly mention uncertainty rather than making unsupported assumptions.\n",
    "\n",
    "    Context:\n",
    "    {truncated_context}\n",
    "    <</SYS>>\n",
    "\n",
    "    Question: {question}\n",
    "    Think step by step, linking each observation across frames to construct a holistic understanding of the video. \n",
    "    Answer the question directly, relying only on the provided context. [/INST]\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"mistral\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"stop\": [\"</s>\", \"[INST]\"]\n",
    "                }\n",
    "            },\n",
    "            timeout=180 \n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"].strip()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "    except KeyError:\n",
    "        return \"Error: Invalid response format\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mistral's Answer:\n",
      "Based on the given transcript and captions, it appears that there are at least six scenarios between the adult (presumably a parent) and the toddler (Daniel) in the video:\n",
      "\n",
      "1. The adult asks Daniel to clean his room (transcript: \"Hey Daniel, would you mind cleaning your room?\").\n",
      "2. The adult instructs Daniel again about cleaning his room (transcript: \"Okay Daniel, time to clean your room\"). Daniel initially refuses (transcript: \"No!\").\n",
      "3. The adult tries to engage with Daniel in a different context (transcript: \"Hi!\").\n",
      "4. The adult informs Daniel that lunch will be ready soon (transcript: \"Lunch will be ready in a minute\").\n",
      "5. The adult asks Daniel what he is doing, possibly while waiting for lunch (transcript: \"Hey Daniel, what's up?\").\n",
      "6. The adult tries to get Daniel's attention and encourages him to look at them (transcript: \"Hey Daniel, look in mommy's eyes\"). The adult also tells the toddler that lunch will be ready soon (transcript: \"Here you go. One more minute and we're gonna have lunch, okay?\").\n",
      "\n",
      "There are no clear indications of additional scenarios between the adult and the toddler in the provided context. However, it is possible that some scenes may have been missed or not described in the captions, such as the actual act of cleaning the room by Daniel or the moment when the adult prepares lunch. Therefore, there might be more than six scenarios, but based on the given information, we can only confirm six scenarios between the adult and the toddler.\n"
     ]
    }
   ],
   "source": [
    "question = \"Did the man show ten scenarios between the adult and the toddler?\"\n",
    "response = ask_mistral2(question, test_video_context)\n",
    "\n",
    "print(\"\\nMistral's Answer:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
